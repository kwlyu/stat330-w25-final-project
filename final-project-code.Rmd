---
title: "Final Project Code"
author: "Kunwu Lyu and Evan Hart"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
library(tidyverse)
library(lme4)
library(ordinal)
library(VGAM)
library(stringr)
library(optimx)
library(car)
library(knitr)
```

## Data Wrangling

```{r Load in Data, cache=TRUE, warning=FALSE, message=FALSE}
## Data from ICPSR
survey <- read_tsv("data/ICPSR_37143/DS0001/37143-0001-Data.tsv") %>%
  janitor::clean_names() # To all lower case
receipt <- read_tsv("data/ICPSR_37143/DS0002/37143-0002-Data.tsv") %>%
  janitor::clean_names()
fast_food <- read_tsv("data/ICPSR_37143/DS0003/37143-0003-Data.tsv") %>%
  janitor::clean_names()
grocery <- read_tsv("data/ICPSR_37143/DS0004/37143-0004-Data.tsv") %>%
  janitor::clean_names()
recall <- read_tsv("data/ICPSR_37143/DS0005/37143-0005-Data.tsv") %>%
  janitor::clean_names()

## Combine multiple surveys
full_data <- survey %>%
  full_join(receipt, relationship = "many-to-many") %>%  
  full_join(fast_food, relationship = "many-to-many") %>%
  full_join(grocery, relationship = "many-to-many") %>%
  full_join(recall, relationship = "many-to-many") 

## Mutating
full_data <- full_data %>% 
  mutate(limit = ordered(q75, levels = c("Never", "Seldom", "Sometimes",
                                         "Often", "Always"))) %>% # for ordinal
  mutate(age = as.numeric(q76),
         gender = if_else(q77 == 0, "M", "F"),
         race = case_when(
           !is.na(q79_1) ~ "Native",
           !is.na(q79_2) ~ "Black",
           !is.na(q79_3) ~ "Asian",
           !is.na(q79_4) ~ "White",
           !is.na(q79_a) ~ "Other"
         ),
         edu = as.numeric(q80),
         location = nemslocationindicator,
         city = q1,
         num_kids = q44,
         surveydate = dmy(surveydate)) %>%
  mutate(days_since_ban = 
           as.numeric(interval(as.Date("2013-03-12"), surveydate) / days(1))) %>% 
  filter(age > 0)
```

```{r Subseting Data, cache=TRUE}
# Standardize numerical for prediction
standardize <- function(x, na.rm = TRUE) {
  (x - mean(x, na.rm = na.rm)) / 
    sd(x, na.rm = na.rm)
}

# subset of complete dataset
reduced_data <- full_data %>% 
  mutate(age_std = standardize(as.numeric(q76))) %>%
  select(c("receiptid", "person_id", "limit", "age", "age_std", "gender",
           "race", "edu", "city", "caff", "location", "round", "nsigns_ssb", 
           "num_kids", "surveydate", "days_since_ban", "caloriescal", "fatg",
           "sugarg")) %>%
  group_by(receiptid) %>% 
  mutate(black = if_else(race == "Black", "Black", "non-Black")) %>% 
  mutate(caff = sum(caff, na.rm = T), # across each receipt
         caloriescal = sum(caloriescal, na.rm = T),
         fatg = sum(fatg, na.rm = T),
         sugarg = sum(sugarg, na.rm = T)) %>% 
  drop_na() %>% 
  distinct() %>% # Remove duplicate rows because multiple items are on a receipt
  mutate(receiptid = as.factor(receiptid),
         person_id = as.factor(person_id),
         location = as.factor(location),
         round = as.factor(round),
         edu = case_when(
           edu == 1 ~ "Less than High School",
           edu == 2 ~ "Some High School",
           edu == 3 ~ "High School",
           edu == 4 ~ "Some College",
           edu == 5 ~ "Associates Degree",
           edu == 6 ~ "College Degree",
           edu == 7 ~ "Graduate Degree"
         )) %>% 
  ungroup() %>% 
  mutate(
         caff_std = standardize(caff),
         nsigns_ssb_std = standardize(nsigns_ssb),
         days_since_ban_std = standardize(days_since_ban),
         caloriescal_std = standardize(caloriescal),
         fatg_std = standardize(fatg),
         sugarg_std = standardize(sugarg)
         ) 

# Cleaned data
write_csv(reduced_data, "dietControl.csv")

# One receipt can't appear in multiple locations
multi_receipt_locations <- reduced_data %>%
  group_by(receiptid) %>% 
  summarize(n_rounds = n_distinct(location)) %>% 
  filter(n_rounds > 1) %>%
  pull(receiptid)

reduced_data %>%
  filter(receiptid %in% multi_receipt_locations) %>%
  count(receiptid, location)
```

## EDA

```{r Bivariate and Interaction, cache=TRUE}
# Single variables, interactions plotted against limit

# Age
ggplot(data = reduced_data, aes(x = age , y = limit)) + 
  geom_boxplot() +
  labs(x = "age", y = "Limit")

# Age faceted by gender
ggplot(data = reduced_data, aes(x = age , y = limit)) + 
  geom_boxplot() +
  facet_wrap(~gender) +
  labs(x = "age", y = "Limit")

# Age faceted by child count
ggplot(data = reduced_data, aes(x = age , y = limit)) + 
  geom_boxplot() +
  facet_wrap(~num_kids) +
  labs(x = "age", y = "Limit")

# Child count
ggplot(data = reduced_data, aes(x = num_kids, fill = limit)) +
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Child Count", y = "Percentage", fill = "Limit") +
  theme_minimal()

# Child count / gender interaction
ggplot(data = reduced_data, aes(x = num_kids, fill = limit)) +
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Child Count", y = "Percentage", fill = "Limit") +
  facet_wrap(~gender) +
  theme_minimal()

# Race
ggplot(data = reduced_data, aes(x = race, fill = limit)) +
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Race", y = "Percentage", fill = "Limit") +
  theme_minimal()

# Education
ggplot(data = reduced_data, aes(x = edu, fill = limit)) +
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Education", y = "Percentage", fill = "Limit") +
  theme_minimal()

# City
ggplot(data = reduced_data, aes(x = city, fill = limit)) +
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "City", y = "Percentage", fill = "Limit") +
  theme_minimal()

# Caffeine
ggplot(data = reduced_data %>% filter(caff > 0), aes(x = caff , y = limit)) + 
  geom_boxplot() +
  labs(x = "Caffeine", y = "Limit")

# Kcal
ggplot(data = reduced_data, aes(x = caloriescal, y = limit)) + 
  geom_boxplot() +
  labs(x = "Calaries", y = "Limit")

# fat
ggplot(data = reduced_data, aes(x = fatg, y = limit)) + 
  geom_boxplot() +
  labs(x = "Fat (g)", y = "Limit")

# sugar
ggplot(data = reduced_data, aes(x = sugarg, y = limit)) + 
  geom_boxplot() +
  labs(x = "Sugar (g)", y = "Limit")

# Survey round
ggplot(data = reduced_data, aes(x = round, fill = limit)) +
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Round", y = "Percentage", fill = "Limit") +
  facet_wrap(~city) +
  theme_minimal()

# Advertisement count
ggplot(data = reduced_data, aes(y = nsigns_ssb, x = limit)) +
  geom_boxplot() + 
  labs(y = "Advertisement Count", x = "Limit") +
  theme_minimal()
```

```{r Programatic Interaction function, cache=TRUE}
# Plot function for interactions

plot_cats <- c("limit", "gender", "race", "city", "round", "num_kids", "edu")
plot_nums <- c("age", "caff", "nsigns_ssb", "days_since_ban", "caloriescal", "sugarg", "fatg")

library(rlang)

make_plot <- function(var1, var2){
  if(var1 %in% plot_cats & var2 %in% plot_cats){
    print(ret_plot <- ggplot(data =reduced_data, aes(x = !!sym(var1), fill = !!sym(var2))) +
      geom_bar(position = "fill") + 
      scale_y_continuous(labels = scales::percent) + 
      theme_minimal())
  }
  
  if(var1 %in% plot_cats & var2 %in% plot_nums){
    print(ret_plot <- ggplot(data = reduced_data, aes(x = !!sym(var1), y = !!sym(var2))) +
      geom_boxplot() + 
      theme_minimal())
  }
  
  if(var1 %in% plot_nums & var2 %in% plot_cats){
    print(ret_plot <- ggplot(data = reduced_data, aes(x = !!sym(var2), y = !!sym(var1))) + 
       geom_boxplot() + 
       theme_minimal())
  }
  
  if(var1 %in% plot_nums & var2 %in% plot_nums){
    print(ggplot(data = reduced_data, aes(x = !!sym(var2), y = !!sym(var1))) +
      geom_point() +
      theme_minimal())
  }
}
```

```{r Call, cache=TRUE}
for(i in 1:length(names(reduced_data))){
  if(i != length(reduced_data)){
    for(j in (i+1):length(reduced_data)){
      make_plot(names(reduced_data)[i], names(reduced_data)[j])
    }
  }
}
```


## Modeling Process

### Testing Different Optimization Methods

For models with no random effects, best to use Newton's approximation. For models with random effects, best to use `nlminb`, which is the default.

```{r Optimization Methods, cache=TRUE, eval=F}
# No random effects
control_clm_full <- clm(limit ~ 1 + age + gender + race + edu + caff +
                          nsigns_ssb + num_kids + days_since_ban, 
                   data = reduced_data, control = list(
  maxIter = 10000, 
  maxLineIter = 2000, 
  maxModIter = 2000, 
  method = "Newton", 
  trace = 1))
control_clm <- clm(limit ~ 1 + age + gender + race + edu + caff + 
                     nsigns_ssb + num_kids + days_since_ban, data = reduced_data, control = list(
  method = "ucminf",
  stepmax = 1,
  grad = "central",
  maxeval = 500000,
  gradstep = c(1e-10, 1e-12),
  trace = 1))
control_clm <- clm(limit ~ 1 + age + gender + race + edu + caff + 
                     nsigns_ssb + num_kids + days_since_ban, data = reduced_data, control = list(
  method = "nlminb",
  eval.max = 2000,
  iter.max = 1500,
  abs.tol = 1e-20,
  trace = 1))
control_clm <- clm(limit ~ 1 + age + gender + race + edu + caff + 
                     nsigns_ssb + num_kids + days_since_ban, data = reduced_data, control = list(
  method = "optim",
  tmax = 100,
  maxit = 100000,
  type = 1,
  ndeps = 1e-10,
  REPORT = 1,
  trace = 1))

## Check with alternative packages. Produced the same intercepts
control_vglm <- vglm(limit ~ 1 + age + gender + race + edu + caff + 
                       nsigns_ssb + num_kids + days_since_ban, 
                     data = reduced_data, family = cumulative(parallel = TRUE))

## Random effects. Omit the rest for brevity
control_clmm_full <- clmm(limit ~ 1 + age + gender + race + edu + city + caff + 
                            nsigns_ssb + num_kids + days_since_ban + 
                            (1 | location) + (1 | round), 
                          control = list(method = "nlminb",
                                         useMatrix = T,
                                         maxIter = 200, 
                                         gradTol = 1e-4, 
                                         maxLineIter = 200,
                                         trace = 1),
                     data = reduced_data, link = "logit")

# Same intercepts
summary(control_clm)
summary(control_vglm)
coef(control_vglm, matrix = T)

summary(control_clmm_full)
coef(control_clmm_full, matrix = T)
```


### Full Model

Note that we also tested the non-standardized model. They both produced the similar conclusions. However, the non-standardized model couldn't fit properly because of the `kcal` variable. We proceeded with the standardized model for predictions.

```{r Full Model, cache=TRUE}
control_clmm_full_std <- clmm(limit ~ 1 + age_std + gender + race + edu + city + caff_std + 
                                nsigns_ssb_std + num_kids + days_since_ban_std + 
                                caloriescal_std + fatg_std + sugarg_std +
                                (1 | location) + (1 | round),
                          control = list(method = "nlminb",
                                         useMatrix = T,
                                         maxIter = 200,
                                         gradTol = 1e-4,
                                         maxLineIter = 200
                                         # , trace = 1
                                         ),
                     data = reduced_data, link = "logit")

summary(control_clmm_full_std)

## Non-standardized model
# control_clmm_full_non <- clmm(limit ~ 1 + age + gender + race + edu + city + caff + 
# nsigns_ssb + num_kids + days_since_ban + kcal + fv + 
# (1 | location) + (1 | round),
#                           control = list(method = "nlminb",
#                                          useMatrix = T,
#                                          maxIter = 200,
#                                          gradTol = 1e-4,
#                                          maxLineIter = 200,
#                                          trace = 1),
#                      data = reduced_data, link = "logit")
```

### Fixed Effects
```{r Full vs. Sig Fixed, cache=TRUE}
control_clmm_red <- clmm(limit ~ 1 + age_std + gender + race + edu + city + 
                           caff_std + num_kids + caloriescal_std +
                                (1 | location) + (1 | round), 
                         data = reduced_data, link = "logit")
anova(control_clmm_red, control_clmm_full_std)
summary(control_clmm_red)
```

### Random Effects

Note that we couldn't perform bootstrap because the `simulate` command is not implemented in `ordinal`, but the effects are fairly marginal and not significant.

#### Level 3 Random Intercept
```{r No Lv3 RE, cache=TRUE}
control_clmm_loc <- clmm(limit ~ 1 + age_std + gender + race + edu + city + 
                           caff_std + num_kids + caloriescal_std +
                           (1 | location),
                         data = reduced_data, link = "logit")

lrt_obs_round <- as.numeric(2*(logLik(control_clmm_red) - 
                                 logLik(control_clmm_loc)))
.5*(1 - pchisq(lrt_obs_round, 0)) + .5*(1 - pchisq(lrt_obs_round, 1))
```

#### Level 2 Random Intercept

```{r No Lv2 RE, cache=TRUE}
control_clm <- clm(limit ~ 1 + age_std + gender + race + edu + city + 
                           caff_std + num_kids + caloriescal_std, 
                   data = reduced_data, link = "logit")
lrt_obs_loc <- as.numeric(2*(logLik(control_clmm_loc) - logLik(control_clm)))
.5*(1 - pchisq(lrt_obs_loc, 0)) + .5*(1 - pchisq(lrt_obs_loc, 1))

summary(control_clmm_loc)
```

### Separate slopes for each level

Ordinal provides two built-in commands for testing whether we need separate slopes for predictors of each level and whether we need to scale our response by each predictors. None of them showed significance.
```{r Separate Slopes, cache=TRUE}
nominal_test(control_clm)
scale_test(control_clm)

control_clm_nom <- clm(limit ~ 1 + race + edu + num_kids + caloriescal_std, 
                       nominal = ~ age_std + gender + city + caff_std, 
                   data = reduced_data, link = "logit")
anova(control_clm_nom, control_clm)
```

#### Overall fit

Compared to the only intercept model.
```{r Overall Chisq test, cache=TRUE}
control_null <- clm(limit ~ 1, data = reduced_data, link = "logit")
# Overall fit
anova(control_null, control_clm_nom)

control_null_re <- clmm(limit ~ 1 + (1 | location), data = reduced_data, link = "logit")
anova(control_null, control_clmm_loc)
```

## Model Diagnostics

### Accuracy Metrics

Because residual analysis are not well understood in ordinal models, we opted for accuracy metrics. Note that our model doesn't predict well.

```{r Model Accuracy, cache=TRUE}
library(tidymodels)
library(workflows)

model_accuracy <- function(model = control_clm, adj = F) {
    comp_metrics <- function(model = model, predict) {
      control_results <- reduced_data %>%
      bind_cols(fit = predict)
    
    
    # Confusion matrix
    # table(control_results$limit, control_results$fit)
    conf_mat(control_results, truth = limit, estimate = fit) -> conf
    
    # accuracy metrics
    accuracy(control_results, truth = limit, estimate = fit) -> acc
    sensitivity(control_results, truth = limit, estimate = fit) -> sen
    specificity(control_results, truth = limit, estimate = fit) -> spe
    # ppv(control_results, truth = limit, estimate = fit)
    
    # Goodness of fit
    chisq.test(control_results$limit, control_results$fit) -> gof
    
    return(list(control_results = control_results, conf = conf, acc = acc, sen = sen, spe = spe, gof = gof))
    }
  if (adj) {
    # Predict response
    control_vglm_pred <- predict(model, type = "response")
    level_counts <- table(reduced_data$limit)
    total_counts <- sum(level_counts)
    proportions <- as.numeric(level_counts / total_counts)
    names(proportions) <- names(level_counts)
    
    adjusted_probs <- control_vglm_pred / proportions[colnames(control_vglm_pred)]
    adjusted_probs <- adjusted_probs / rowSums(adjusted_probs)
    fit <- ordered(colnames(adjusted_probs)[max.col(adjusted_probs)],
                     levels = c("Never", "Seldom", "Sometimes",
                                "Often", "Always"))
    comp_metrics(model = model, predict = fit) -> result
  } else {
    # Predict response
    control_pred <- predict(model, type = "class")
    comp_metrics(model = model, control_pred) -> result
  }
  return(result)
}

model_accuracy(control_clm_nom)

## Similar results under different model specifications
control_clm_probit <- clm(limit ~ 1 + age_std + gender + race + edu + city + 
                           caff_std + num_kids + caloriescal_std, 
                   data = reduced_data, link = "probit")
model_accuracy(control_clm_probit)

control_clm_sym <- clm(limit ~ 1 + age_std + gender + race + edu + city + 
                           caff_std + num_kids + caloriescal_std, 
                   data = reduced_data, 
                   link = "probit", threshold = "equidistant")
model_accuracy(control_clm_sym)


## Use VGAM to get prob for each level of resp, not implemented in Ordinal
## Similarly inaccurate model
control_vglm_sig <- vglm(limit ~ 1 + age_std + gender + race + edu + city + 
                           caff_std + num_kids + caloriescal_std, 
                     data = reduced_data, 
                     family = cumulative(parallel = TRUE))
model_accuracy(control_vglm_sig, adj = T)
```


```{r Residual Analysis, cache=TRUE}
plot(fitted(control_clm), residuals(control_clm))

control_resid <- model_accuracy(control_clm_nom)$control_results %>%
  mutate(
    case_id = row_number(), 
    fit = ordered(fit, levels = c("Never", "Seldom", "Sometimes", "Often", "Always"))
  ) %>% 
  select(case_id, limit, fit) %>% 
  mutate(
    limit_num = case_when(
      limit == "Never" ~ 0,
      limit == "Seldom" ~ 1,
      limit == "Sometimes" ~ 2,
      limit == "Often" ~ 3,
      limit == "Always" ~ 4),
    fit_num = case_when(
      fit == "Never" ~ 0,
      fit == "Seldom" ~ 1,
      fit == "Sometimes" ~ 2,
      fit == "Often" ~ 3,
      fit == "Always" ~ 4),
    ) %>% 
  mutate(resid = limit_num - fit_num)

ggplot(control_resid, aes(x = case_id)) +
  geom_jitter(aes(y = limit, color = "Actual"), alpha = 0.4) +
  geom_jitter(aes(y = fit, color = "Fitted"), alpha = 0.4) +
  scale_color_manual(values = c("Actual" = "blue", "Fitted" = "red")) +
  labs(
    x = "Case ID",
    y = "Response Category",
    title = "Actual vs. Fitted Values",
    color = "Legend"
  ) +
  theme_minimal()

ggplot(control_resid, aes(x = case_id, y = resid)) +
  geom_jitter(alpha = 0.4) +
  labs(
    x = "Case ID",
    y = "Residual"
  ) +
  theme_minimal()
```

## Effects Interpretation

### Confidence Intervals

```{r CI}
confint(control_clmm_loc) %>% kable(digits = 3)
exp(confint(control_clmm_loc)) %>% kable(digits = 3)
(100*(exp(confint(control_clmm_loc))-1)) %>% kable(digits = 3)
```

